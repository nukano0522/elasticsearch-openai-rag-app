{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nukan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cpu\n",
      "4.35.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=\"./backend/api/ca.crt\",\n",
    "    basic_auth=(\"elastic\", \"elastic\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"vector-test-index-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nukan\\AppData\\Local\\Temp\\ipykernel_31780\\1503124373.py:39: DeprecationWarning: Importing from the 'elasticsearch.client' module is deprecated. Instead use 'elasticsearch' module for importing the client.\n",
      "  from elasticsearch.client import IngestClient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パイプラインの設定データ\n",
    "# data = {\n",
    "#     \"description\": \"Text embedding pipeline\",\n",
    "#     \"processors\": [\n",
    "#         {\n",
    "#             \"inference\": {\n",
    "#                 \"model_id\": \"cl-tohoku__bert-base-japanese-v2\",\n",
    "#                 \"target_field\": \"text_embedding\",\n",
    "#                 \"field_map\": {\n",
    "#                     \"title\": \"text_field\"\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "data = {\n",
    "    \"description\": \"Text embedding pipeline\",\n",
    "    \"processors\": [\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": \"cl-tohoku__bert-base-japanese-v2\",\n",
    "                \"input_output\": [\n",
    "                    {\n",
    "                        \"input_field\": \"title\",\n",
    "                        \"output_field\": \"title_embedding\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"input_field\": \"text\",\n",
    "                        \"output_field\": \"text_embedding\"\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# https://elasticsearch-py.readthedocs.io/en/v8.14.0/api/ingest-pipelines.html\n",
    "from elasticsearch.client import IngestClient\n",
    "IngestClient(es).put_pipeline(id=\"japanese-text-embeddings\", body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'vector-test-index-01'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexの作成\n",
    "# body = {\n",
    "#   \"mappings\": {\n",
    "#     \"properties\": {\n",
    "#       \"text_embedding.predicted_value\": {\n",
    "#         \"type\": \"dense_vector\",\n",
    "#         \"dims\": 768,\n",
    "#         \"index\": True,\n",
    "#         \"similarity\": \"cosine\"\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "\n",
    "body = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 768,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"text_embedding\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 768,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# すでにindexが存在する場合は削除\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es_a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # サンプルドキュメントの定義\n",
    "# sample_doc = {\n",
    "#     \"title\": \"日本経済の現状\",\n",
    "#     \"text\": \"日本経済は現在、回復基調にありますが、依然として世界的な不確実性に直面しています。\"\n",
    "# }\n",
    "# # ドキュメントのインデックス登録\n",
    "# response = es.index(\n",
    "#     index=index_name,  # 事前に作成したインデックス\n",
    "#     body=sample_doc,\n",
    "#     pipeline=\"japanese-text-embeddings\"  # 事前に設定したパイプライン\n",
    "# )\n",
    "\n",
    "# # 結果の確認\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nukan\\AppData\\Local\\Temp\\ipykernel_31780\\3659037656.py:34: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  bulk(es, bulk_insert(docs), chunk_size=1000, request_timeout=180)\n"
     ]
    },
    {
     "ename": "BulkIndexError",
     "evalue": "571 document(s) failed to index.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBulkIndexError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 41\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBulk insert completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# print(\"bulk insert start.\")\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# bulk(es, bulk_insert(docs), request_timeout=180)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# print(\"bulk insert end.\")\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mexecute_bulk_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m index_count \u001b[38;5;241m=\u001b[39m es\u001b[38;5;241m.\u001b[39mcount(index\u001b[38;5;241m=\u001b[39mindex_name)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_count[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 34\u001b[0m, in \u001b[0;36mexecute_bulk_insert\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_bulk_insert\u001b[39m(docs):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Elasticsearch(\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://localhost:9200\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m         ca_certs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./backend/api/ca.crt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m         basic_auth\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melastic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melastic\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     33\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m es:\n\u001b[1;32m---> 34\u001b[0m         \u001b[43mbulk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbulk_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBulk insert completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nukan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elasticsearch\\helpers\\actions.py:521\u001b[0m, in \u001b[0;36mbulk\u001b[1;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[0;32m    520\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myield_ok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ok, item \u001b[38;5;129;01min\u001b[39;00m streaming_bulk(\n\u001b[0;32m    522\u001b[0m     client, actions, ignore_status\u001b[38;5;241m=\u001b[39mignore_status, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    523\u001b[0m ):\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;66;03m# go through request-response pairs and detect failures\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stats_only:\n",
      "File \u001b[1;32mc:\\Users\\nukan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elasticsearch\\helpers\\actions.py:436\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[1;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(max_backoff, initial_backoff \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (attempt \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 436\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, (ok, info) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    437\u001b[0m         bulk_data,\n\u001b[0;32m    438\u001b[0m         _process_bulk_chunk(\n\u001b[0;32m    439\u001b[0m             client,\n\u001b[0;32m    440\u001b[0m             bulk_actions,\n\u001b[0;32m    441\u001b[0m             bulk_data,\n\u001b[0;32m    442\u001b[0m             raise_on_exception,\n\u001b[0;32m    443\u001b[0m             raise_on_error,\n\u001b[0;32m    444\u001b[0m             ignore_status,\n\u001b[0;32m    445\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    447\u001b[0m         ),\n\u001b[0;32m    448\u001b[0m     ):\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    450\u001b[0m             action, info \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mpopitem()\n",
      "File \u001b[1;32mc:\\Users\\nukan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elasticsearch\\helpers\\actions.py:355\u001b[0m, in \u001b[0;36m_process_bulk_chunk\u001b[1;34m(client, bulk_actions, bulk_data, raise_on_exception, raise_on_error, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _process_bulk_chunk_success(\n\u001b[0;32m    350\u001b[0m         resp\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    351\u001b[0m         bulk_data\u001b[38;5;241m=\u001b[39mbulk_data,\n\u001b[0;32m    352\u001b[0m         ignore_status\u001b[38;5;241m=\u001b[39mignore_status,\n\u001b[0;32m    353\u001b[0m         raise_on_error\u001b[38;5;241m=\u001b[39mraise_on_error,\n\u001b[0;32m    354\u001b[0m     )\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "File \u001b[1;32mc:\\Users\\nukan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\elasticsearch\\helpers\\actions.py:274\u001b[0m, in \u001b[0;36m_process_bulk_chunk_success\u001b[1;34m(resp, bulk_data, ignore_status, raise_on_error)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m ok, {op_type: item}\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[1;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BulkIndexError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m document(s) failed to index.\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors)\n",
      "\u001b[1;31mBulkIndexError\u001b[0m: 571 document(s) failed to index."
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "doc_name = \"jawikinews-20240805-cirrussearch-content.json.gz\"\n",
    "\n",
    "def bulk_insert(docs):\n",
    "    for doc in docs:\n",
    "        yield {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"text\": doc[\"text\"]\n",
    "            },\n",
    "            \"pipeline\": \"japanese-text-embeddings\"\n",
    "        }\n",
    "        \n",
    "docs = []\n",
    "with gzip.open(f\"./backend/api/data/{doc_name}\") as f:\n",
    "    for line in f:\n",
    "        json_line = json.loads(line)\n",
    "        if \"index\" not in json_line:\n",
    "            doc = json_line\n",
    "            docs.append(doc)\n",
    "\n",
    "\n",
    "# バルクインサートの処理\n",
    "def execute_bulk_insert(docs):\n",
    "    with Elasticsearch(\n",
    "        \"https://localhost:9200\",\n",
    "        ca_certs=\"./backend/api/ca.crt\",\n",
    "        basic_auth=(\"elastic\", \"elastic\"),\n",
    "    ) as es:\n",
    "        bulk(es, bulk_insert(docs), chunk_size=1000, request_timeout=180)\n",
    "        print(\"Bulk insert completed.\")\n",
    "\n",
    "# print(\"bulk insert start.\")\n",
    "# bulk(es, bulk_insert(docs), request_timeout=180)\n",
    "# print(\"bulk insert end.\")\n",
    "\n",
    "execute_bulk_insert(docs)\n",
    "\n",
    "index_count = es.count(index=index_name)\n",
    "print(f\"Indexed {index_count['count']} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3940"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# from elasticsearch.helpers import bulk\n",
    "\n",
    "# import asyncio\n",
    "# from elasticsearch import AsyncElasticsearch\n",
    "# from elasticsearch.helpers import async_streaming_bulk, BulkIndexError\n",
    "# import nest_asyncio\n",
    "\n",
    "# # nest_asyncioを適用\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# doc_name = \"jawikinews-20240805-cirrussearch-content.json.gz\"\n",
    "\n",
    "# es_a = AsyncElasticsearch(\n",
    "#     \"https://localhost:9200\",\n",
    "#     ca_certs=\"./backend/api/ca.crt\",\n",
    "#     basic_auth=(\"elastic\", \"elastic\"),\n",
    "# )\n",
    "\n",
    "# async def gendata(docs):\n",
    "#     for doc in docs:\n",
    "#         yield {\n",
    "#             \"_op_type\": \"index\",\n",
    "#             \"_index\": index_name,\n",
    "#             \"_source\": {\n",
    "#                 \"title\": doc[\"title\"],\n",
    "#                 \"text\": doc[\"text\"]\n",
    "#             },\n",
    "#             \"pipeline\": \"japanese-text-embeddings\"\n",
    "#         }\n",
    "        \n",
    "# docs = []\n",
    "# with gzip.open(f\"./backend/api/data/{doc_name}\") as f:\n",
    "#     for line in f:\n",
    "#         json_line = json.loads(line)\n",
    "#         if \"index\" not in json_line:\n",
    "#             doc = json_line\n",
    "#             docs.append(doc)\n",
    "\n",
    "# async def main():\n",
    "#     try:\n",
    "#         async for ok, result in async_streaming_bulk(client=es_a,\n",
    "#                                                     actions=gendata(docs),\n",
    "#                                                     chunk_size=50,  # 一度に扱うドキュメント数\n",
    "#                                                     max_chunk_bytes=5428800  # 一度に扱うバイト数\n",
    "#                                                     ):\n",
    "#             # 各チャンクごとの実行結果を取得\n",
    "#             action, result = result.popitem()\n",
    "#             # バルクインサートに失敗した場合\n",
    "#             if not ok:\n",
    "#                 print(f\"failed to {result} document {action}\")\n",
    "\n",
    "\n",
    "#     # 例外処理\n",
    "#     except BulkIndexError as bulk_error:\n",
    "#         # エラーはリスト形式\n",
    "#         print(bulk_error.errors)\n",
    "\n",
    "#     # セッションのクローズ\n",
    "#     await es_a.close()\n",
    "\n",
    "# # イベントループを取得\n",
    "# loop = asyncio.get_event_loop()\n",
    "# # 並列に実行して終るまで待つ\n",
    "# loop.run_until_complete(main())\n",
    "\n",
    "# index_count = es.count(index=index_name)\n",
    "# print(f\"Indexed {index_count['count']} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 6.59474, Document: ('野球で延長タイブレーク制導入へ 国際野球連盟', '【2008年7月27日】 毎日新聞によるとIBAF国際野球連盟のハービー・シラー会長は、野球の延長戦の試合時間短縮を目的としてタイブレーク制度を、8月に予定されている北京五輪から採用することを発表した。 IBAFによると、タイブレーク制度はソフトボールで行っている方式と同じもので、延長戦10回は通常通りのルールで試合をするが、11回の攻撃は任意の打順からノーアウト1・2塁のランナーの段階で試合を行い、12回以後は前のイニングスの続きの打順から同じくノーアウト1・2塁の段階で試合をするというものであるが、このルール変更は日本野球連盟への事前連絡がなく、突然7月26日（日本時間\\u3000UTC+9）になってメールで報告が行われた。 しかし、読売新聞によると日本代表・星野仙一監督らはこれに反対する意見を出し、日本のプロ・アマの野球界全体で抗議する事を決めた。星野氏は「五輪の2週間前になってルールを変えるだなんておかしいにも程がある。我々は親善試合ではなく世界一を決める真剣勝負をするんだ。どこにも相談無しにIBAFが決めたのも納得できない」として抗議する姿勢を強く打ち出した。また、全日本アマチュア野球連盟もこの決定を確認した上で抗議を行う方向であるという。 また毎日新聞によると、シラー氏は7月30日に来日し、日本野球連盟・松田昌士会長やプロ野球・加藤良三コミッショナーと会談。その際に日本側から今回の問題についての説明を求めるものと見られている。 毎日jp 『国際野球連盟：タイブレーク採用\\u3000日本関係者から困惑の声』 —\\xa0毎日新聞, 2008年7月27日 YOMIURI ONLINE（北京五輪） 『五輪野球のタイブレーク制導入、日本はプロ・アマ挙げて抗議』 —\\xa0読売新聞, 2008年7月26日')\n",
      "Score: 5.6271515, Document: ('プロ野球・パ・リーグ 参加球団のウェブサイトを一括管理へ', '【2008年1月25日】 日本経済新聞と時事通信によると、プロ野球パ・リーグの事業会社である「パシフィック・リーグ・マーケティング（PLM社）」は1月24日、参加6チームがこれまで個別に運営していたパソコンと携帯電話用の公式ウェブサイトを一括管理し、1月31日から新しいウェブサイトを立ち上げることを明らかにした。 日経によると、今回の試みについてPLM社の代表取締役で北海道日本ハムの藤井純一球団社長は「アメリカでのリーグビジネスの成功例を参考に、日本を代表するIT産業企業をスポンサーに持つチームが参加するパ・リーグの特色を融合させた」と説明している。 また時事通信によると、2007年度まではオリックス以外の参加5チームが行っていた携帯電話用の有料動画サービス「プロ野球24」を今年度はパ・リーグ参加6チーム全てで行うことや、パソコン向けインターネット動画配信についても、ソフトバンクの関連会社「GTエンターテインメント社」を通して全チーム・全試合を「Yahoo!動画」にて配信することになった。なおGTE社は北海道日本ハム以外の参加5チームのCS衛星放送・ケーブルテレビ向け中継の独占放映権を2008年・2009年の2年間取得している。 プロ野球パ・リーグ ソフトバンク子会社にCS独占放映権売却 NIKKEI NET 『公式ＨＰなどを一括管理\\u3000パ６球団の共同事業会社』 —\\xa0日本経済新聞, 2008年1月24日 時事ドットコム 『携帯サイトを一括管理＝ＰＬＭ』 —\\xa0時事通信社, 2008年1月24日')\n",
      "Score: 5.6271515, Document: ('韓国野球・現代ユニコーンズ 投資ファンド会社へ球団譲渡', '【2008年2月1日】 韓国の東亜日報、朝鮮日報によると、韓国プロ野球リーグに所属する現代（ヒョンデ、またはヒュンダイ）ユニコーンズの経営権譲渡問題で、リーグ戦を主催する韓国野球委員会（KBO）は1月30日、ソウルの「野球会館」で投資ファンド会社のセンテニアル・インベストメント社に譲渡すること（現地では新チーム設立と表記）を決め、調印式を行った。 東亜日報によると、KBOのシン・サンウ総裁は「センテニアル社は新規参入に際し加盟料金120億ウォンを委員会に払って、本拠地はソウルの木洞球場を使う」ことを発表。またセンテニアル社はチームの運営会社を保有するが、チーム名についてはいわゆる命名権を採用し、チーム名を毎年変えるのを防ぐためにメインのスポンサーについては複数年の長期契約を求めるという。メインスポンサーの企業はユニホームの前面にロゴを入れ、年間90～120億ウォン、サブスポンサーはヘルメットなどに広告を入れて5～10億ウォンで契約するとしている。朝鮮日報の報道によると、スポンサーには少なくとも3年間前後の契約を前提に進め、大企業を中心に3・4社程度と契約を結ぶつもりである」と示唆した。 また、東亜日報によるとハ・イルソン事務総長は「安定したチーム運営を目指すために1月15日、センテニアル社と了解覚え書きを交わした際、5年間は球団譲渡禁止、選手のトレードはKBOと事前に協議するなどの条項を加えている」と説明した。 朝鮮日報によると、当面の本拠地は木洞球場を使用するが、2010年に完成する予定の高尺洞ハーフドームの竣工後はこのドーム球場を使用する。また東亜日報と中央日報によると、参加チームや選手協会は今回の球団譲渡先の決定を歓迎。選手協会は「シン総裁をはじめKBOの役員、ファン、指導者の関心と苦労に感謝」と話している。またLGツインズ球団のキム・ヨンス社長は「投資ファンドがプロ野球参入で新しい試みをするのはいいが、スポンサーだけで安定した経営が出来るかは未知数」と疑問視する目もある。 朝鮮日報によると、センテニアル社はM&A（企業買収・合併）や企業の戦略諮問が中心で、2007年に資本金5000万ウォンで設立したばかり。本社をソウル市太平路にあるファイナンスセンターに置き、日本とベトナムにも現地法人がある。 \"韓国野球・現代ユニコーンズ解散か KTへの売却白紙に\"。ウィキニュース、2008年1月17日 CHOSUN ONLINE\\u3000陳仲彦（チン・ジュンオン） 『野球：センテニアルがプロ参入、8球団体制維持へ（上）』 —\\xa0朝鮮日報, 2008年1月31日 CHOSUN ONLINE\\u3000陳仲彦（チン・ジュンオン） 『同上（下）』 —\\xa0朝鮮日報, 2008年1月31日 CHOSUN ONLINE\\u3000陳仲彦（チン・ジュンオン） 『野球：新規参入センテニアルへの期待と不安』 —\\xa0朝鮮日報, 2008年1月31日 donga.com 『プロ野球新生球団は投資会社センテニアル\\u3000１２０億支払い縁故地はソウル』 —\\xa0東亜日報, 2008年1月31日 『選手協、 センテニアルの現代引受決定を歓迎』 —\\xa0中央日報, 2008年1月31日')\n",
      "Score: 5.5215845, Document: ('2007年プロ野球高校生ドラフト会議', '【2007年10月12日】 10月3日、日本野球機構は高校生を対象としたドラフト会議（高校生選択会議）を開催し、総勢39名に対する契約交渉権の確定を発表した。 中日新聞によると、今回の高校生ドラフトで最大の注目だった中田翔（大阪桐蔭）は4チーム競合となる抽選の末に北海道日本ハムが、また佐藤由規（仙台育英）は現在の高校生の部で最多の5チームの抽選で東京ヤクルトがそれぞれ交渉権を得た。今回のドラフトは全体で39人となり、これは過去2回の最多だった2005年の38人を上回っている。 （出典・日本野球機構）：ウエイバー順にて各リーグを掲載 広島東洋カープ\\u3000安部友裕（福岡工大城東・内野手） 東京ヤクルトスワローズ\\u3000佐藤由規（仙台育英・投手） 横浜ベイスターズ\\u3000田中健二朗（常葉菊川・投手） 阪神タイガース\\u3000高濱卓也（横浜・内野手） 中日ドラゴンズ\\u3000赤坂和幸（浦和学院・投手） 読売ジャイアンツ\\u3000藤村大介（熊本工・投手） オリックス・バファローズ\\u3000丹羽将弥（岐阜城北・外野手） 東北楽天ゴールデンイーグルス\\u3000寺田龍平（札幌南・投手） 西武ライオンズ\\u3000（辞退） 千葉ロッテマリーンズ\\u3000唐川侑己（成田・投手） 福岡ソフトバンクホークス\\u3000岩嵜翔（市立船橋・投手） 北海道日本ハムファイターズ\\u3000中田翔（大阪桐蔭・外野手） また、11月19日に予定されている「大学生・社会人ほか選択会議」で1巡目指名を回避（行わない）する球団がないことから、今回の高校生ドラフトでは2巡目指名は行われなかった。 NPBニュース 『2007年ドラフト会議（高校生選択会議）』 —\\xa0日本野球機構, 2007年10月3日 『2007年 新人選手選択会議（ドラフト会議）』 —\\xa0日本野球機構, 2007年10月12日閲覧 CHUNICHI Web（スポーツ） 『竜１巡目は浦和学院・赤坂\\u3000プロ野球・高校生ドラフト会議』 —\\xa0中日新聞, 2007年10月4日')\n",
      "Score: 5.5215845, Document: ('2008プロ野球・パ・リーグの日程発表', '【2007年12月7日】 プロ野球・パ・リーグは12月6日、2008年度リーグ戦の日程を発表した。 時事通信と日経新聞によると、開幕戦は3月20日で、2006年度のリーグ戦上位3チームがそれぞれ主催する北海道日本ハム対千葉ロッテ、埼玉西武対オリックス、福岡ソフトバンク対東北楽天イーグルスの3試合がそれぞれ3連戦で予定されているが、祝日の翌日となる21日は開催せず23日まで行う変則日程となった。 2008年度のリーグ戦も144試合で争われるが、パ・リーグ参加6チームとの間では24回総当り・120試合、交流試合（セ・リーグ参加6チームとの対戦）は4回総当り・24試合である。また北京五輪の準決勝・決勝の開催される予定の8月22・23日は開催しない。 また、パ・リーグによると2008年度の公式戦で久しぶりに行われる地方球場は2箇所で、県営大宮公園球場では1954年9月の毎日（千葉ロッテ）対近鉄（消滅）以来54年ぶりに埼玉西武対千葉ロッテの6月27日の試合を、また熊本藤崎台球場では1989年6月の福岡ダイエー（ソフトバンク）対日本ハム（北海道日本ハム）以来19年ぶり（2007年度にも予定されていたが雨天中止のため）となる福岡ソフトバンク対オリックス戦の4月8日の試合がそれぞれ予定されている。なお上位3チームによるクライマックスシリーズの日程は後日発表される。 2008プロ野球セ・パ交流試合の日程決まる 『2008年度公式戦日程』 —\\xa0日本野球機構, 2007年12月6日 NIKKEI NET 『開幕は３月２０日\\u3000パ・リーグが日程発表』 —\\xa0日本経済新聞, 2007年12月6日 時事ドットコム 『パは３月２０日開幕＝来季の日程発表－プロ野球』 —\\xa0時事通信社, 2007年12月6日')\n",
      "Score: 5.5215845, Document: ('日本プロ野球・クライマックスシリーズ2008にアドバンテージ導入', '【2008年3月5日】 読売新聞によると、プロ野球で2007年度から導入された各リーグ上位3チームによるプレーオフ制度・クライマックスシリーズについて、1回戦を勝ち上がったチームとリーグペナントレース（交流試合込み144試合）の優勝チームとで行う2回戦（一般のトーナメントの決勝戦相当）を2007年度の5戦3勝制から、ペナントレース優勝チームに対するアドバンテージ1勝分付きの6戦制に変更されることが3月4日にあった実行委員会で決まった。 読売と朝日新聞によると、1回戦は前年と同じくペナントレース2位と3位の球団で3戦2勝制。その勝者とペナントレース優勝チームによる2回戦はペナントレース優勝チームは3勝、1回戦勝ち抜けチームは4勝すれば日本シリーズ出場権を得る。試合会場は従来と同じく対戦するチームのペナントレース上位チーム（1回戦は2位チーム、2回戦は優勝チーム）の本拠球場で全て行う。なおリーグ優勝の扱いは前年と同じくペナントレースの1位チームとなる。 なお、両紙によると2007年度のセ・リーグのクライマックスシリーズは2回戦でリーグ優勝の巨人軍が中日に3連敗し、シリーズ出場権を逃していた。そのことから、ペナントレースで優勝したチームをより優位にする改正案を1月にセ・リーグ側が提案し、この日の実行委員会の席でパ・リーグもこれに同意した。これについては巨人軍が一度2007年度に反対していたが、今年度はそれを付けるべきと主張していた。 YOMIURI ONLINEスポーツ 『「リーグＶチームに１勝」決定…ＣＳ第２ステージ』 —\\xa0読売新聞, 2008年3月4日 asahi.com 『クライマックスＳ第２Ｓ、アドバンテージ付き６試合制に』 —\\xa0朝日新聞, 2008年3月4日')\n",
      "Score: 5.374565, Document: ('韓国野球新チームの名称は「ウリ・ヒーローズ」', '【2008年3月8日】 韓国の朝鮮日報によると、韓国プロ野球リーグに新規参入するセンテニアル・インベストメントを運営の母体とする球団のチーム名がウリ・ヒーローズとなることが2月28日決まった。 朝鮮日報の別記事によると、センテニアル社は2月21日に韓国のタバコメーカー・ウリタバコ社と3年間・300億ウォン（日本円レートで34億円相当）のメインスポンサー契約を締結。チーム名のネーミングやユニホーム、ヘルメット・帽子などに広告を掲出する権利を与えた。またスポンサー後援料は選手やフロントの毎月の給料日の前に入金されて、安定した経営が出来るという。ウリ社は2006年に韓国初の民間資本のタバコ会社であり、2007年12月に財政経済部からの事業認可も得ている。 2月28日ソウル市で行われた球団命名式で、ウリ社は企業の社会的倫理と公益性を踏まえて正式社名の「タバコ」の文字は入れず、チーム名を「ウリ・ヒーローズ」とすることを決めた。またこの日はユニホームのお披露目もあり、「飛翔する英雄」をイメージした強い男性をイメージしたデザインが施された。チームカラーはワイン色である。 しかし、聯合ニュースの報道によると、韓国禁煙運動協議会は2月21日に声明で「タバコ会社がスポーツ（のイベントや団体）を後援することは健康な精神と肉体を目指すスポーツ精神に外れるだけでなく、スポーツに対する冒涜（ぼうとく）行為の点で絶対あってはならない」と反対する姿勢を示した。WHO世界保健機関が定めて韓国の政府も批准した「タバコ規制枠組み条約」においても、タバコメーカーがスポーツ関連の後援は出来ないように勧告しているとともに、ウリ社の球団支援はやめなければいけないと主張している。また、韓国の政府に対しても「タバコの入った言葉のある企業がスポーツ関連の後援を一切出来ないように強制・禁止する法律を速やかに制定するように」求めている。 『野球：新生球団の正式名称は「ウリ・ヒーローズ」』 —\\xa0朝鮮日報, 2008年2月29日 高錫泰（コ・ソクテ） 『野球：センテニアル、たばこ会社とスポンサー契約』 —\\xa0朝鮮日報, 2008年2月22日 Yahoo!ニュース 『禁煙運動協議会、ウリたばこのプロ野球後援に反対』 —\\xa0聯合ニュース, 2008年2月22日')\n",
      "Score: 5.374565, Document: ('日本プロ野球・オリックスのコリンズ監督辞任', '【2008年5月22日】 時事通信によると、日本プロ野球・オリックス・バファローズのテリー・コリンズ監督が、5月21日に神戸スカイマーク球場であった阪神タイガース戦終了後、会見し、「自分の中で限界も見えて、野球をするための情熱がなくなった」としてシーズン途中での監督辞任を明らかにした。今後は大石大二郎ヘッド兼内野守備走塁コーチが「監督代行」に就任する。 また毎日新聞によると、これに合わせて投手・打者のそれぞれのチーフコーチを担当していたマイク・ブラウン、ジョン・ディーバスの両氏も退団することが決まった。オリックスのシーズン中の監督交代は2003年途中の石毛宏典氏以来の事で、この年以後の6年間で6回監督が交代したこととなる。 毎日新聞によると、コリンズ氏はアメリカの大リーグの複数のチームで指揮をとった後、2006年中村勝広元監督（現・球団本部長）から後継を受ける形でオリックス球団として2人目の外国人監督として就任。3年契約を結んでいたが、2007年度はリーグ戦3年ぶりの最下位。2008年も4月11日から5月20日に脱するまで最下位に低迷する状況が続いていた。 読売新聞によると、コリンズ氏は5月16日に中村本部長に辞任の意向を伝えた。21日の試合終了後の会見でコリンズ氏は「監督は強い情熱を持っていないといけないが、メラメラ燃えるものがなくなった」とコメントしている。 時事ドットコム 『オリックス・コリンズ監督が辞任＝大石ヘッドが監督代行』 —\\xa0時事通信社, 2008年5月21日(UTC+9) 毎日jp 『オリックス：コリンズ監督の辞任発表…本人が辞意伝える』 —\\xa0毎日新聞, 2008年5月21日23時58分(UTC+9) YOMIURI ONLINEスポーツ 『成績不振、オリックスのコリンズ監督辞任…監督代行に大石コーチ』 —\\xa0読売新聞, 2008年5月22日(UTC+9)')\n",
      "Score: 5.374565, Document: ('プロ野球パ・リーグ、埼玉西武レギュラーシーズン優勝', '【2008年9月27日】 時事通信によると、プロ野球・パ・リーグは9月26日（UTC+9）、埼玉西武ライオンズが4年ぶり16回目（西鉄ライオンズ時代を含めると21回目）のペナントレース（レギュラーシーズン）優勝を決めた。 この日、埼玉西武は札幌ドームでの北海道日本ハム戦に0-2で敗れたものの、マジックの対象チームで、東北楽天イーグルスと対戦（クリネックス宮城球場）したオリックスが1-13で敗れたことによりリーグ戦優勝が決まったものである。 47NEWS（共同通信）によると、埼玉西武は2007年度は5位に低迷して上位3チームを指す「Aクラス」入賞が連続25年で途絶えたが、今年は若手の野手を中心にした活躍で4月6日以後首位を一度も明け渡さなかった。 毎日新聞によると、渡辺久信監督は優勝後のインタビューで「選手たちは今年1年間頑張った。開幕当初は勝てなかったときもあったが、その後は一戦一戦づつ力を付けて強いチームになった。昨年（2007年。ペナントレース5位）の秋から思うとまさかこの位置（ペナントレース優勝）にいるとは思わなかった。辛い練習と汗と涙の結晶で忘れられない優勝」と喜びを語った。 各新聞によると、埼玉西武は10月17日スタートのクライマックスシリーズ・パの2回戦（対1回戦勝ち上がりチームとの6戦4勝制。埼玉西武にはアドバンテージ1勝分が付くため、実質3勝）に優勝すれば、2004年以来の日本シリーズ出場権獲得となる。 時事ドットコム 『西武、４年ぶりのＶ＝プロ野球パ・リーグ』 —\\xa0時事通信社, 2008年9月26日21時14分(UTC+9) 共同通信 『西武、４年ぶりリーグ制覇 渡辺監督で５位から頂点へ』 —\\xa047NEWS, 2008年9月26日21時3分(UTC+9) 毎日jp 『西武：４年ぶりリーグ優勝…渡辺監督、１年目で頂点に導く』 —\\xa0毎日新聞社, 2008年9月26日23時57分(UTC+9)')\n",
      "Score: 5.102828, Document: ('2007年プロ野球社会人・大学生ドラフト会議', '【2007年11月26日】 日本野球機構によると、2007年プロ野球ドラフト会議の社会人・大学生その他の部が11月19日に都内であった。今回のドラフトでは34人が指名を受けて、高校生の部（10月3日）の39人を含め73人の交渉権が確定した。また引き続いて育成選手ドラフトでは15人がプロ各チームから指名を受けた。 朝日新聞によると、今回のドラフトにおいて注目された「大学野球の投手ビッグ3」のうち、大場翔太（東洋大）は6チーム競合による抽選の末福岡ソフトバンクが、また長谷部康平（愛知工大）は東北楽天イーグルス、加藤幹典（慶応大）は東京ヤクルトがそれぞれ第1巡で指名をした。 なお今回の社会人・大学生ドラフトでは2巡目指名が全球団なかった。これは高校生ドラフトの1巡目を回避した球団に対して行われるが、今回は高校ドラフトの1巡目を全チーム行ったためによる。 各リーグのウェーバー順に掲載。出典は日刊スポーツ。 東京ヤクルト\\u3000加藤幹典（慶応大学・投手） 広島\\u3000篠田純平（日本大学・投手） 横浜\\u3000小林太志（JR東日本・投手） 阪神\\u3000白仁田寛和（福岡大学・投手） 中日\\u3000山内壮馬（名城大学・投手） 巨人軍\\u3000村田透（大阪体育大学・投手） オリックス\\u3000小林賢司（青山学院大学・投手） 西武\\u3000平野将光（JR東日本東北・投手） 東北楽天\\u3000長谷部康平（愛知工業大学・投手） 福岡ソフトバンク\\u3000大場翔太（東洋大学・投手） 千葉ロッテ\\u3000服部泰卓（トヨタ自動車・投手） 北海道日本ハム\\u3000多田野数人（アメリカマイナーリーグ・トリプルA・サクラメント・投手） NPBニュース 『2007年ドラフト（大学生・社会人ほか）』 —\\xa0日本野球機構, 2007年11月19日 nikkansports.com 『2007年社会人・大学ドラフトの指名選手一覧表』 —\\xa0日刊スポーツ, 2007年11月19日 asahi.com 『大場はソフトバンク、長谷部は楽天が交渉権\\u3000１巡目確定』 —\\xa0朝日新聞, 2007年11月19日')\n"
     ]
    }
   ],
   "source": [
    "WORD = \"野球\"\n",
    "\n",
    "# knn_query = {\n",
    "#   \"query\": {\n",
    "#     \"match\": {\n",
    "#       \"title\": {\n",
    "#         \"query\": WORD,\n",
    "#         \"boost\": 0.9\n",
    "#       }\n",
    "#     }\n",
    "#   },\n",
    "#   \"knn\": [ {\n",
    "#     \"field\": \"title_embedding\",\n",
    "#     \"query_vector_builder\": {\n",
    "#         \"text_embedding\": {\n",
    "#             \"model_id\": \"cl-tohoku__bert-base-japanese-v2\",\n",
    "#             \"model_text\": WORD,\n",
    "#         }\n",
    "#     },\n",
    "#     \"k\": 5,\n",
    "#     \"num_candidates\": 50,\n",
    "#     \"boost\": 0.1\n",
    "#   },\n",
    "#   {\n",
    "#     \"field\": \"text_embedding\",\n",
    "#     \"query_vector_builder\": {\n",
    "#         \"text_embedding\": {\n",
    "#             \"model_id\": \"cl-tohoku__bert-base-japanese-v2\",\n",
    "#             \"model_text\": WORD,\n",
    "#         }\n",
    "#     },\n",
    "#     \"k\": 10,\n",
    "#     \"num_candidates\": 10,\n",
    "#     \"boost\": 0.5\n",
    "#   }],\n",
    "#   \"size\": 10\n",
    "# }\n",
    "\n",
    "with open(\"./backend/api/query/bert_vector_search_01.json\", \"r\") as f:\n",
    "  query_data = f.read()\n",
    "\n",
    "replace_dict = {\n",
    "    \"{WORD}\": WORD\n",
    "}\n",
    "for key, value in replace_dict.items():\n",
    "    query_data = query_data.replace(key, value)\n",
    "\n",
    "query_data = json.loads(query_data)\n",
    "\n",
    "# KNN検索の実行\n",
    "response = es.search(\n",
    "    index=index_name,\n",
    "    body=query_data\n",
    ")\n",
    "\n",
    "# 結果の表示\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Score: {hit['_score']}, Document: {hit['_source']['title'], hit['_source']['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknn_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nukan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "source": [
    "json.loads(knn_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
